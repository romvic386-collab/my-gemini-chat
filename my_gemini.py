import streamlit as st
import google.generativeai as genai
from PIL import Image # –ü–æ–¥–∫–ª—é—á–∞–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫—É –¥–ª—è –∫–∞—Ä—Ç–∏–Ω–æ–∫
import time

# --- –ù–ê–°–¢–†–û–ô–ö–ò –°–¢–†–ê–ù–ò–¶–´ ---
st.set_page_config(page_title="Gemini Vision", page_icon="üëÅÔ∏è", layout="centered")

# --- –ì–õ–û–ë–ê–õ–¨–ù–ê–Ø –ü–ê–ú–Ø–¢–¨ ---
@st.cache_resource
def get_global_history():
    return []

global_history = get_global_history()

# --- –ë–û–ö–û–í–ê–Ø –ü–ê–ù–ï–õ–¨ ---
with st.sidebar:
    st.header("üéõÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
    
    # 1. –í–´–ë–û–† –ö–õ–Æ–ß–ê
    available_keys = [k for k in st.secrets.keys() if k.startswith("KEY_")]
    if available_keys:
        selected_key_name = st.selectbox(
            "üîë –í—ã–±–µ—Ä–∏ –∫–ª—é—á:",
            options=available_keys,
            format_func=lambda x: f"–ö–ª—é—á #{x.split('_')[1]}"
        )
        API_KEY = st.secrets[selected_key_name]
    else:
        st.error("–î–æ–±–∞–≤—å –∫–ª—é—á–∏ –≤ Secrets!")
        st.stop()

    # 2. –í–´–ë–û–† –ú–û–î–ï–õ–ò
    # –î–ª—è –∫–∞—Ä—Ç–∏–Ω–æ–∫ –ª—É—á—à–µ –≤—Å–µ–≥–æ —Ä–∞–±–æ—Ç–∞—é—Ç 1.5 Flash –∏ 3.0 Pro (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞)
    selected_model = st.radio(
        "üß† –ú–æ–¥–µ–ª—å:",
        options=["gemini-2.5-flash", "gemini-2.5-pro", "gemini-3-pro-preview"], 
        index=0 # Flash –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–±—ã—Å—Ç—Ä–∞—è –∏ —Ö–æ—Ä–æ—à–æ –≤–∏–¥–∏—Ç)
    )
    
    st.divider()

    # üî• 3. –ó–ê–ì–†–£–ó–ö–ê –§–û–¢–û üî•
    st.header("üñºÔ∏è –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–æ—Ç–æ")
    uploaded_file = st.file_uploader("–ö–∏–¥–∞–π –∫–∞—Ä—Ç–∏–Ω–∫—É —Å—é–¥–∞:", type=["jpg", "png", "jpeg", "webp"])
    
    image_to_send = None
    if uploaded_file is not None:
        # –û—Ç–∫—Ä—ã–≤–∞–µ–º –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞—Ä—Ç–∏–Ω–∫—É
        image_to_send = Image.open(uploaded_file)
        st.image(image_to_send, caption="–ì–æ—Ç–æ–≤–æ –∫ –æ—Ç–ø—Ä–∞–≤–∫–µ", use_container_width=True)
        st.success("–§–æ—Ç–æ –ø—Ä–∏–∫—Ä–µ–ø–ª–µ–Ω–æ! –¢–µ–ø–µ—Ä—å –ø–∏—à–∏ –∑–∞–ø—Ä–æ—Å –≤ —á–∞—Ç.")

    st.divider()
    
    # 4. –£–ü–†–ê–í–õ–ï–ù–ò–ï
    if st.button("üóëÔ∏è –°—Ç–µ—Ä–µ—Ç—å –∏—Å—Ç–æ—Ä–∏—é"):
        global_history.clear()
        st.rerun()

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
genai.configure(api_key=API_KEY)

# --- –ò–ù–¢–ï–†–§–ï–ô–° –ß–ê–¢–ê ---
st.title(f"üëÅÔ∏è –ß–∞—Ç ({selected_model})")

if "messages" not in st.session_state:
    st.session_state.messages = global_history

# –û—Ç—Ä–∏—Å–æ–≤–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏
for message in global_history:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–ü–†–û–°–ê ---
if prompt := st.chat_input("–ù–∞–ø–∏—à–∏ –≤–æ–ø—Ä–æ—Å –ø–æ –∫–∞—Ä—Ç–∏–Ω–∫–µ..."):
    
    # 1. –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    # –ï—Å–ª–∏ –µ—Å—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫–∞, –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ–º–µ—Ç–∫—É –≤ —Ç–µ–∫—Å—Ç –∏—Å—Ç–æ—Ä–∏–∏
    user_text_display = prompt
    if image_to_send:
        user_text_display = f"üñºÔ∏è [–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ —Ñ–æ—Ç–æ] {prompt}"

    global_history.append({"role": "user", "content": user_text_display})
    
    with st.chat_message("user"):
        st.markdown(user_text_display)
        # –ï—Å–ª–∏ –∫–∞—Ä—Ç–∏–Ω–∫–∞ –µ—Å—Ç—å —Å–µ–π—á–∞—Å, –ø–æ–∫–∞–∂–µ–º –µ—ë –≤ —á–∞—Ç–µ —Ç–æ–∂–µ
        if image_to_send:
            st.image(image_to_send, width=300)

    # 2. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        try:
            model = genai.GenerativeModel(selected_model)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å: –¢–µ–∫—Å—Ç + –ö–∞—Ä—Ç–∏–Ω–∫–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å) –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –¢–µ–∫—Å—Ç
            if image_to_send:
                request_content = [prompt, image_to_send]
            else:
                request_content = prompt

            # –°—Ç—Ä–∏–º–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞
            response = model.generate_content(request_content, stream=True)
            
            for chunk in response:
                if chunk.text:
                    full_response += chunk.text
                    message_placeholder.markdown(full_response + "‚ñå")
                    time.sleep(0.01)
            
            message_placeholder.markdown(full_response)
            global_history.append({"role": "assistant", "content": full_response})
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞: {e}")
